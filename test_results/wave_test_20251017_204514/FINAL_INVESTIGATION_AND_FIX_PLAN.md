# üîç –§–ò–ù–ê–õ–¨–ù–û–ï –†–ê–°–°–õ–ï–î–û–í–ê–ù–ò–ï: Duplicate Positions Bug
## –ì–ª—É–±–æ–∫–∏–π –ê–Ω–∞–ª–∏–∑ + –ü–ª–∞–Ω –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è

**Investigation ID**: DUPLICATE_POS_FINAL_001
**Date**: 2025-10-17
**Status**: ‚úÖ ROOT CAUSE IDENTIFIED - FIX PLAN READY

---

## üìã EXECUTIVE SUMMARY

### –ù–∞—Ö–æ–¥–∫–∏

1. ‚úÖ **Root Cause Identified**: –î–≤–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ–∑–¥–∞—é—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã
2. ‚úÖ **–ú–∞—Å—à—Ç–∞–±**: 6 –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö –ø–∞—Ä (12 –∑–∞–ø–∏—Å–µ–π total) –≤ production
3. ‚úÖ **Impact**: –¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ –ë–î, —Ç–æ—Ä–≥–æ–≤–ª—è –Ω–µ –∑–∞—Ç—Ä–æ–Ω—É—Ç–∞
4. ‚úÖ **Fix Complexity**: Medium (—Ç—Ä–µ–±—É–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –¥–≤—É—Ö —Å–∏—Å—Ç–µ–º)

### –ö–ª—é—á–µ–≤—ã–µ –ü—Ä–æ–±–ª–µ–º—ã

| # | –ü—Ä–æ–±–ª–µ–º–∞ | Severity | Complexity |
|---|----------|----------|------------|
| 1 | `create_position()` –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã | üî¥ Critical | Low |
| 2 | –î–≤–∞ sync —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ | üî¥ Critical | Medium |
| 3 | `self.positions` –≤—ã—Ö–æ–¥–∏—Ç –∏–∑ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ | üü° High | Medium |
| 4 | –ù–µ—Ç DB constraint –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã | üü° High | Low |

---

## üî¨ –î–ï–¢–ê–õ–¨–ù–û–ï –†–ê–°–°–õ–ï–î–û–í–ê–ù–ò–ï

### –ß–∞—Å—Ç—å 1: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏

#### –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –î–í–ê –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ç–æ—Ä–∞:

**Synchronizer #1: PositionSynchronizer**
- **–§–∞–π–ª**: `core/position_synchronizer.py`
- **–ö–æ–≥–¥–∞**: –ü—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –±–æ—Ç–∞ (–æ–¥–∏–Ω —Ä–∞–∑)
- **–ú–µ—Ç–æ–¥**: `synchronize_all_exchanges()` ‚Üí `_add_missing_position()`
- **–°–æ–∑–¥–∞–Ω–∏–µ**: `repository.open_position()` ‚Üí `create_position()`
- **–ü—Ä–æ–≤–µ—Ä–∫–∞**: –°–º–æ—Ç—Ä–∏—Ç –Ω–∞ `db_map` (–∏–∑ –ë–î) vs `exchange_map`

**Synchronizer #2: PositionManager.sync_exchange_positions()**
- **–§–∞–π–ª**: `core/position_manager.py:573`
- **–ö–æ–≥–¥–∞**: –ö–∞–∂–¥—ã–µ ~2.5 –º–∏–Ω—É—Ç—ã (periodic)
- **–ú–µ—Ç–æ–¥**: `sync_exchange_positions()` ‚Üí creates position
- **–°–æ–∑–¥–∞–Ω–∏–µ**: `repository.create_position()` –ù–ê–ü–†–Ø–ú–£–Æ
- **–ü—Ä–æ–≤–µ—Ä–∫–∞**: –°–º–æ—Ç—Ä–∏—Ç –Ω–∞ `self.positions` (–≤ –ø–∞–º—è—Ç–∏) vs exchange

#### –ö–æ–Ω—Ñ–ª–∏–∫—Ç:

```
PositionSynchronizer:
  ‚îú‚îÄ –ß–∏—Ç–∞–µ—Ç –ë–î
  ‚îú‚îÄ –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å –±–∏—Ä–∂–µ–π
  ‚îî‚îÄ –°–æ–∑–¥–∞—ë—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ (–≤ –ë–î)

PositionManager.sync:
  ‚îú‚îÄ –ß–∏—Ç–∞–µ—Ç self.positions (–ø–∞–º—è—Ç—å)
  ‚îú‚îÄ –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å –±–∏—Ä–∂–µ–π
  ‚îî‚îÄ –°–æ–∑–¥–∞—ë—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ (–≤ –ø–∞–º—è—Ç–∏) ‚ùå –ù–ï –ü–†–û–í–ï–†–Ø–ï–¢ –ë–î!
```

---

### –ß–∞—Å—Ç—å 2: –ü–æ—á–µ–º—É `self.positions` –¢–µ—Ä—è–µ—Ç –ó–∞–ø–∏—Å–∏?

#### Timeline Analysis (TAGUSDT Case):

```
19:21:07 - Wave Processor —Å–æ–∑–¥–∞—ë—Ç –ø–æ–∑–∏—Ü–∏—é #583
19:21:07 - –î–æ–±–∞–≤–ª—è–µ—Ç –≤ self.positions['TAGUSDT']
19:21:07 - –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ –ë–î (position_id=583)

[–ß—Ç–æ-—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∑–¥–µ—Å—å...]

19:23:18 - Periodic sync –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
19:23:18 - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç: 'TAGUSDT' not in self.positions ‚Üí TRUE ‚ùå
19:23:18 - –ü–æ–ª—É—á–∞–µ—Ç —Å –±–∏—Ä–∂–∏: TAGUSDT exists
19:23:18 - –°–æ–∑–¥–∞—ë—Ç –¥—É–±–ª–∏–∫–∞—Ç #589
```

#### Hypothesis Testing:

**–ì–∏–ø–æ—Ç–µ–∑–∞ #1: Position Removed by Cleanup**
```python
# –ò—â–µ–º: –≥–¥–µ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è self.positions.pop()
# –†–µ–∑—É–ª—å—Ç–∞—Ç: –¢–æ–ª—å–∫–æ –≤ sync_exchange_positions():665
self.positions.pop(pos_state.symbol, None)  # –ü—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ orphaned
```
**–í—ã–≤–æ–¥**: TAGUSDT –ù–ï –±—ã–ª closed, —Ç–∞–∫ —á—Ç–æ —ç—Ç–∞ –ø—Ä–∏—á–∏–Ω–∞ –∏—Å–∫–ª—é—á–µ–Ω–∞ ‚ùå

**–ì–∏–ø–æ—Ç–µ–∑–∞ #2: self.positions NOT Populated Yet**
```python
# –ü—Ä–æ–≤–µ—Ä–∏–º: –∫–æ–≥–¥–∞ self.positions –∑–∞–ø–æ–ª–Ω—è–µ—Ç—Å—è?
# position_manager.py:523-571
async def load_positions_from_database(self):
    """Load all active positions from database"""
    positions = await self.repository.get_open_positions()
    for pos in positions:
        position_state = PositionState(...)
        self.positions[pos['symbol']] = position_state
```

**Evidence from Logs**:
```
20:10:47 - Loading positions from database...
20:10:47 - PositionSynchronizer starts
20:10:47 - Found 17 positions in database
20:10:47 - Found 14 positions on exchange
20:10:47 - Skipped: GLMRUSDT not in tracked positions ([])...
```

**Smoking Gun**: `tracked positions ([])` = –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫!

**–í—ã–≤–æ–¥**: `self.positions` –±—ã–ª –ü–£–°–¢ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ! ‚úÖ –ë–ò–ù–ì–û!

**–ì–∏–ø–æ—Ç–µ–∑–∞ #3: Race Between Load and Sync**

–ü—Ä–æ–≤–µ—Ä–∏–º –ø–æ—Ä—è–¥–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏:
```python
# main.py –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π
await position_manager.load_positions_from_database()  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤ self.positions
await synchronizer.synchronize_all_exchanges()  # –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç self.positions
await position_manager.start_periodic_sync()  # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç self.positions
```

**Evidence**:
```
20:10:47,136 - Loading positions from database...
20:10:47,137 - Synchronizing positions with exchanges...  ‚Üê –°–†–ê–ó–£!
20:10:47,151 - Found 17 positions in database  ‚Üê PositionSynchronizer
20:10:47,412 - Skipped: GLMRUSDT not in tracked positions ([])  ‚Üê –ü–£–°–¢–û!
```

**Timeline**:
- T+0ms: `load_positions_from_database()` –≤—ã–∑–≤–∞–Ω
- T+1ms: `synchronizer.synchronize_all_exchanges()` –≤—ã–∑–≤–∞–Ω
- T+15ms: PositionSynchronizer —á–∏—Ç–∞–µ—Ç –ë–î (17 positions)
- T+276ms: Position updates –ø—Ä–∏—Ö–æ–¥—è—Ç, –Ω–æ `self.positions = []`

**–ü—Ä–æ–±–ª–µ–º–∞**: `load_positions_from_database()` –ù–ï –î–û–ñ–ò–î–ê–ï–¢–°–Ø –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è!

---

### –ß–∞—Å—Ç—å 3: –ö–æ–¥ Repository - No Duplicate Check

#### create_position() (database/repository.py:208)

```python
async def create_position(self, position_data: Dict) -> int:
    """Create new position record"""
    query = """
        INSERT INTO monitoring.positions (
            symbol, exchange, side, quantity,
            entry_price, status
        ) VALUES ($1, $2, $3, $4, $5, 'active')
        RETURNING id
    """

    async with self.pool.acquire() as conn:
        position_id = await conn.fetchval(
            query,
            position_data['symbol'],
            position_data['exchange'],
            position_data['side'],
            position_data['quantity'],
            position_data['entry_price']
        )
        return position_id
```

**–ü—Ä–æ–±–ª–µ–º–∞**: ‚ùå –ù–ï–¢ –ü–†–û–í–ï–†–ö–ò –ù–ê –î–£–ë–õ–ò–ö–ê–¢–´!

**–ß—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å**:
```python
async def create_position(self, position_data: Dict) -> int:
    # ‚úÖ –ü–†–û–í–ï–†–ò–¢–¨: —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ active –ø–æ–∑–∏—Ü–∏—è?
    existing = await self.get_open_position(
        position_data['symbol'],
        position_data['exchange']
    )

    if existing:
        logger.warning(f"Position {symbol} already exists (id={existing['id']})")
        return existing['id']  # –í–µ—Ä–Ω—É—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é

    # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç
    position_id = await conn.fetchval(...)
    return position_id
```

---

### –ß–∞—Å—Ç—å 4: Database - No Constraint

#### Current Schema:

```sql
CREATE TABLE monitoring.positions (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(20),
    exchange VARCHAR(50),
    status VARCHAR(20),
    -- ... other fields
);
```

**–ü—Ä–æ–±–ª–µ–º–∞**: ‚ùå –ù–ï–¢ UNIQUE constraint –Ω–∞ (symbol, exchange, status)!

**–ß—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å**:
```sql
-- Prevent multiple active positions for same symbol
ALTER TABLE monitoring.positions
ADD CONSTRAINT unique_active_position_per_symbol
EXCLUDE (symbol WITH =, exchange WITH =)
WHERE (status = 'active');
```

–ò–ª–∏ –ø—Ä–æ—â–µ:
```sql
CREATE UNIQUE INDEX idx_unique_active_position
ON monitoring.positions (symbol, exchange)
WHERE status = 'active';
```

---

## üéØ ROOT CAUSES IDENTIFIED

### Root Cause #1: Async Race Condition at Startup

**Problem**:
```python
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
await position_manager.load_positions_from_database()  # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
await synchronizer.synchronize_all_exchanges()  # –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –î–û –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è load

# –†–µ–∑—É–ª—å—Ç–∞—Ç:
# - load_positions() –ù–ï –∑–∞–≤–µ—Ä—à–∏–ª –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ self.positions
# - synchronizer —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
# - synchronizer –ù–ï –≤–∏–¥–∏—Ç –ø–æ–∑–∏—Ü–∏–π –≤ self.positions (–ø—É—Å—Ç)
# - synchronizer –Ω–∞—Ö–æ–¥–∏—Ç –∏—Ö –Ω–∞ –±–∏—Ä–∂–µ
# - synchronizer —Å–æ–∑–¥–∞—ë—Ç –≤ –ë–î ‚Üê –î–£–ë–õ–ò–ö–ê–¢ #1
```

**Evidence**:
- Logs –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç: `tracked positions ([])` = –ø—É—Å—Ç–æ
- Synchronizer –Ω–∞—à—ë–ª 17 –≤ –ë–î, 14 –Ω–∞ –±–∏—Ä–∂–µ
- –ù–æ `self.positions` –±—ã–ª –ø—É—Å—Ç

### Root Cause #2: Periodic Sync Doesn't Check Database

**Problem**:
```python
# position_manager.py:679
if symbol not in self.positions:  # ‚ùå –¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏!
    position_id = await self.repository.create_position({...})  # –°–æ–∑–¥–∞—ë—Ç –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ë–î
```

**Why self.positions Empty?**:

**Scenario A**: Position Created After Last Load
```
T+0: Bot loads positions ‚Üí self.positions populated
T+60: Wave creates new position #583
T+60: Adds to self.positions['TAGUSDT']
T+120: Something removes from self.positions (restart? cleanup?)
T+150: Periodic sync runs
T+150: TAGUSDT not in self.positions ‚Üí creates duplicate
```

**Scenario B**: self.positions Out of Sync
```
T+0: Position #583 created (by wave or manual)
T+60: Position still in –ë–î and on exchange
T+120: self.positions cleared/reloaded without #583
T+150: Periodic sync: TAGUSDT not in memory ‚Üí duplicate
```

### Root Cause #3: No Database-Level Protection

**Problem**: `create_position()` –ø—Ä–æ—Å—Ç–æ –¥–µ–ª–∞–µ—Ç INSERT –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏

**Why Bad**:
- –ù–µ—Ç atomic check-and-create
- –ù–µ—Ç UNIQUE constraint –≤ –ë–î
- –ù–µ—Ç IntegrityError –ø—Ä–∏ –¥—É–±–ª–∏–∫–∞—Ç–µ

---

## üìä IMPACT ANALYSIS

### Current Damage

**Duplicates Found**: 6 pairs (12 records)

| Symbol | Real ID | Ghost ID | Age | Orders | Trades |
|--------|---------|----------|-----|--------|--------|
| AKTUSDT | 578 | 581 | ~16s | 2 vs 0 | 1 vs 0 |
| HEIUSDT | 577 | 580 | ~19s | 2 vs 0 | 1 vs 0 |
| COTIUSDT | 585 | 590 | ~2min | 2 vs 0 | 1 vs 0 |
| HOLOUSDT | 584 | 588 | ~1.5min | 2 vs 0 | 1 vs 0 |
| LSKUSDT | 586 | 587 | ~25s | 2 vs 0 | 1 vs 0 |
| TAGUSDT | 583 | 589 | ~2min | 2 vs 0 | 1 vs 0 |

**Pattern**:
- Real: Created first, has exchange_order_id, has orders/trades
- Ghost: Created later by sync, NO exchange_order_id, NO orders/trades

### Trading Impact: ‚úÖ NONE

- ‚ùå No double trading (only 1 real position per symbol)
- ‚úÖ Stop-losses —Ä–∞–±–æ—Ç–∞—é—Ç (–Ω–∞ real position)
- ‚úÖ P&L calculations correct (ghost not used)

### Data Impact: üî¥ HIGH

- ‚ùå Database polluted (12 extra records)
- ‚ùå Metrics inflated (66 positions vs 60 actual)
- ‚ùå MAX_POSITIONS bypass potential
- ‚ùå Confusion in queries

---

## üîß FIX PLAN

### Phase 1: IMMEDIATE HOTFIX (Priority 0)

#### Fix 1.1: Add DB Check to create_position()

**File**: `database/repository.py:208`

**Change**:
```python
async def create_position(self, position_data: Dict) -> int:
    """Create new position record"""
    symbol = position_data['symbol']
    exchange = position_data['exchange']

    # ‚úÖ CHECK: Does active position already exist?
    existing = await self.get_open_position(symbol, exchange)
    if existing:
        logger.warning(
            f"‚ö†Ô∏è Position {symbol} already exists in DB (id={existing['id']}). "
            f"Returning existing position instead of creating duplicate."
        )
        return existing['id']

    # Create new position only if doesn't exist
    query = """
        INSERT INTO monitoring.positions (
            symbol, exchange, side, quantity,
            entry_price, status
        ) VALUES ($1, $2, $3, $4, $5, 'active')
        RETURNING id
    """

    async with self.pool.acquire() as conn:
        position_id = await conn.fetchval(
            query,
            symbol,
            exchange,
            position_data['side'],
            position_data['quantity'],
            position_data['entry_price']
        )

        logger.info(f"‚úÖ Created new position: {symbol} (id={position_id})")
        return position_id
```

**Impact**: üî¥ **CRITICAL FIX** - prevents all future duplicates

**Risk**: üü¢ **LOW** - only adds check, doesn't change logic

**Testing**: Simple - try to create same position twice, should return same ID

---

#### Fix 1.2: Add DB Check to position_manager.sync

**File**: `core/position_manager.py:679`

**Before (buggy)**:
```python
if symbol not in self.positions or self.positions[symbol].exchange != exchange_name:
    # Creates without checking DB
    position_id = await self.repository.create_position({...})
    self.positions[symbol] = position_state
```

**After (fixed)**:
```python
if symbol not in self.positions or self.positions[symbol].exchange != exchange_name:
    # ‚úÖ CHECK DATABASE FIRST
    existing_position = await self.repository.get_open_position(symbol, exchange_name)

    if existing_position:
        # Position exists in DB but not in memory - RESTORE it
        position_state = PositionState(
            id=existing_position['id'],
            symbol=symbol,
            exchange=exchange_name,
            side=side,
            quantity=quantity,
            entry_price=entry_price,
            current_price=entry_price,
            unrealized_pnl=0,
            unrealized_pnl_percent=0,
            has_stop_loss=existing_position.get('has_stop_loss', False),
            stop_loss_price=existing_position.get('stop_loss_price'),
            has_trailing_stop=existing_position.get('has_trailing_stop', False),
            trailing_activated=False,
            opened_at=existing_position.get('opened_at') or datetime.now(timezone.utc),
            age_hours=0
        )

        self.positions[symbol] = position_state
        logger.info(f"‚ôªÔ∏è Restored existing position from DB: {symbol} (id={existing_position['id']})")

        # Continue with stop-loss check...
    else:
        # Position truly doesn't exist - CREATE new one
        position_id = await self.repository.create_position({...})
        # ... rest of creation logic
```

**Impact**: üî¥ **CRITICAL FIX** - handles out-of-sync self.positions

**Risk**: üü° **MEDIUM** - changes sync logic

**Testing**: Moderate - simulate position in DB but not in memory

---

### Phase 2: STRUCTURAL FIX (Priority 1)

#### Fix 2.1: Fix Startup Race Condition

**File**: `main.py` (–∏–ª–∏ –≥–¥–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)

**Before**:
```python
# –ó–∞–ø—É—Å–∫–∞—é—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
await position_manager.load_positions_from_database()
await synchronizer.synchronize_all_exchanges()
```

**After**:
```python
# ‚úÖ SEQUENTIAL: –¥–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä–µ–¥ sync
logger.info("Loading positions from database...")
await position_manager.load_positions_from_database()
logger.info(f"Loaded {len(position_manager.positions)} positions into memory")

logger.info("Synchronizing with exchanges...")
await synchronizer.synchronize_all_exchanges()
logger.info("Synchronization complete")
```

**Impact**: üî¥ **CRITICAL FIX** - prevents startup duplicates

**Risk**: üü¢ **LOW** - just adds ordering

**Testing**: Check logs for correct sequence

---

#### Fix 2.2: Use DB Instead of self.positions for db_symbols

**File**: `core/position_manager.py:603`

**Before**:
```python
db_symbols = {s for s, p in self.positions.items() if p.exchange == exchange_name}
```

**After**:
```python
# ‚úÖ QUERY DATABASE DIRECTLY for db_symbols
db_positions_list = await self.repository.get_open_positions()
db_symbols = {
    p['symbol'] for p in db_positions_list
    if p.get('exchange') == exchange_name
}
logger.info(f"üîç DEBUG db_symbols from DB ({len(db_symbols)}): {sorted(db_symbols)}")
```

**Impact**: üü° **HIGH FIX** - makes db_symbols accurate

**Risk**: üü¢ **LOW** - just changes data source

**Testing**: Verify db_symbols matches actual DB

---

### Phase 3: DATABASE PROTECTION (Priority 1)

#### Fix 3.1: Add UNIQUE Constraint

**Migration Script**: `migrations/add_unique_active_position_constraint.sql`

```sql
-- Prevent duplicate active positions
CREATE UNIQUE INDEX IF NOT EXISTS idx_unique_active_position
ON monitoring.positions (symbol, exchange)
WHERE status = 'active';

-- This creates a partial unique index that only applies to active positions
-- Multiple closed/rolled_back positions for same symbol are allowed
```

**Impact**: üî¥ **CRITICAL PROTECTION** - DB-level enforcement

**Risk**: üü° **MEDIUM** - might break existing code that expects duplicates

**Testing**: Try INSERT duplicate active position, should get IntegrityError

**Rollback Plan**:
```sql
DROP INDEX IF EXISTS idx_unique_active_position;
```

---

#### Fix 3.2: Handle IntegrityError in Code

**File**: `database/repository.py:create_position()`

**Add**:
```python
from asyncpg.exceptions import UniqueViolationError

async def create_position(self, position_data: Dict) -> int:
    # ... check code from Fix 1.1 ...

    try:
        position_id = await conn.fetchval(query, ...)
        return position_id

    except UniqueViolationError as e:
        # Duplicate caught by DB constraint
        logger.warning(
            f"‚ö†Ô∏è Duplicate position prevented by DB constraint: {symbol}. "
            f"Fetching existing position."
        )

        # Fetch and return existing
        existing = await self.get_open_position(symbol, exchange)
        if existing:
            return existing['id']
        else:
            # Should never happen, but handle gracefully
            logger.error(f"UniqueViolation but get_open_position returned None for {symbol}")
            raise
```

**Impact**: üü¢ **DEFENSE IN DEPTH** - graceful handling

**Risk**: üü¢ **LOW** - only adds error handling

---

### Phase 4: CLEANUP (Priority 2)

#### Fix 4.1: Identify and Mark Ghost Positions

**Script**: `scripts/cleanup_duplicate_positions.py`

```python
async def identify_ghost_positions():
    """Find ghost positions (no orders, no trades)"""
    query = """
        SELECT
            p.id,
            p.symbol,
            p.exchange,
            p.entry_price,
            p.created_at,
            p.exchange_order_id,
            COUNT(DISTINCT o.id) as orders_count,
            COUNT(DISTINCT t.id) as trades_count
        FROM monitoring.positions p
        LEFT JOIN monitoring.orders o ON o.position_id = p.id::text
        LEFT JOIN monitoring.trades t ON t.order_id = o.order_id
        WHERE p.status = 'active'
        GROUP BY p.id
        HAVING COUNT(DISTINCT o.id) = 0
        ORDER BY p.created_at DESC
    """

    ghosts = await repository.fetch(query)

    print(f"Found {len(ghosts)} potential ghost positions:")
    for ghost in ghosts:
        print(f"  #{ghost['id']} {ghost['symbol']} - "
              f"created {ghost['created_at']}, "
              f"order_id={ghost['exchange_order_id']}")

    return ghosts
```

---

#### Fix 4.2: Close Ghost Positions

**Script continues**:
```python
async def close_ghost_positions(ghosts, dry_run=True):
    """Close identified ghost positions"""
    for ghost in ghosts:
        position_id = ghost['id']
        symbol = ghost['symbol']

        # Double-check: really a ghost?
        orders = await repository.fetch(
            "SELECT id FROM monitoring.orders WHERE position_id = $1",
            str(position_id)
        )

        if len(orders) > 0:
            print(f"  SKIP #{position_id} {symbol} - has {len(orders)} orders (NOT a ghost)")
            continue

        if dry_run:
            print(f"  [DRY RUN] Would close #{position_id} {symbol}")
        else:
            await repository.close_position(
                position_id=position_id,
                close_price=ghost.get('entry_price', 0),
                pnl=0,
                pnl_percentage=0,
                reason='GHOST_CLEANUP'
            )
            print(f"  ‚úÖ Closed ghost #{position_id} {symbol}")

# Usage:
ghosts = await identify_ghost_positions()
await close_ghost_positions(ghosts, dry_run=True)  # First test
# await close_ghost_positions(ghosts, dry_run=False)  # Then execute
```

---

### Phase 5: MONITORING (Priority 3)

#### Fix 5.1: Add Duplicate Detection Metric

**File**: `core/position_manager.py`

**Add**:
```python
class PositionManager:
    def __init__(self, ...):
        # ... existing code ...
        self.metrics = {
            'duplicates_prevented': 0,
            'duplicates_detected': 0
        }

    async def sync_exchange_positions(self, exchange_name: str):
        # ... existing code ...

        if symbol not in self.positions:
            existing = await self.repository.get_open_position(symbol, exchange_name)

            if existing:
                # DUPLICATE DETECTED!
                self.metrics['duplicates_prevented'] += 1
                logger.error(
                    f"üö® DUPLICATE PREVENTED: {symbol} already exists in DB "
                    f"(id={existing['id']}) but not in memory. "
                    f"This indicates self.positions is out of sync."
                )

                # Restore to memory
                # ... restoration code ...
```

---

#### Fix 5.2: Add Periodic Duplicate Check

**File**: `core/health_checker.py` (or similar)

**Add**:
```python
async def check_for_duplicate_positions():
    """Periodic check for duplicate active positions"""
    query = """
        SELECT
            symbol,
            exchange,
            COUNT(*) as count,
            ARRAY_AGG(id ORDER BY created_at) as position_ids
        FROM monitoring.positions
        WHERE status = 'active'
        GROUP BY symbol, exchange
        HAVING COUNT(*) > 1
    """

    duplicates = await repository.fetch(query)

    if duplicates:
        logger.error(f"üö® FOUND {len(duplicates)} DUPLICATE POSITIONS!")
        for dup in duplicates:
            logger.error(
                f"  {dup['symbol']}: {dup['count']} positions "
                f"(IDs: {dup['position_ids']})"
            )

        # Send alert
        await send_alert(
            title="Duplicate Positions Detected",
            message=f"Found {len(duplicates)} symbols with duplicate positions",
            severity="HIGH"
        )

    return len(duplicates)

# Run every 5 minutes
asyncio.create_task(periodic_check(check_for_duplicate_positions, interval=300))
```

---

## üìã IMPLEMENTATION PLAN

### Step 1: Testing & Validation (Day 1)

1. ‚úÖ **Verify current duplicates**
   ```sql
   SELECT symbol, exchange, COUNT(*) as count
   FROM monitoring.positions
   WHERE status = 'active'
   GROUP BY symbol, exchange
   HAVING COUNT(*) > 1;
   ```

2. ‚úÖ **Identify ghost positions**
   ```python
   python scripts/cleanup_duplicate_positions.py --dry-run
   ```

3. ‚úÖ **Test fix in dev environment**
   - Apply Fix 1.1 (DB check in create_position)
   - Try to create duplicate
   - Verify: returns existing ID

---

### Step 2: Hotfix Deployment (Day 1-2)

**Deploy Order**:
1. Fix 1.1: Add DB check to `create_position()` ‚úÖ HIGH PRIORITY
2. Fix 1.2: Add DB check to `sync_exchange_positions()` ‚úÖ HIGH PRIORITY
3. Fix 2.1: Fix startup race condition ‚úÖ HIGH PRIORITY
4. Fix 2.2: Use DB for db_symbols ‚úÖ MEDIUM PRIORITY

**Deployment Steps**:
```bash
# 1. Backup database
pg_dump fox_crypto > backup_before_fix.sql

# 2. Apply code changes
git pull origin fix/duplicate-positions
git checkout fix/duplicate-positions

# 3. Restart bot
./scripts/stop_bot_safely.sh
./scripts/start_bot_safely.sh

# 4. Monitor logs
tail -f logs/production_bot_*.log | grep -E "duplicate|DUPLICATE|WARNING"
```

---

### Step 3: Database Protection (Day 2-3)

**Deploy Order**:
1. Fix 3.1: Add UNIQUE constraint ‚úÖ HIGH PRIORITY
2. Fix 3.2: Handle IntegrityError ‚úÖ MEDIUM PRIORITY

**Migration Steps**:
```bash
# 1. Check for existing duplicates first
psql -c "SELECT ..." # Use query from Step 1

# 2. Clean up existing duplicates
python scripts/cleanup_duplicate_positions.py --execute

# 3. Apply constraint
psql -f migrations/add_unique_active_position_constraint.sql

# 4. Verify constraint
psql -c "\d monitoring.positions"
```

---

### Step 4: Cleanup (Day 3-4)

**Execute**:
1. Fix 4.1: Identify all ghosts
2. Fix 4.2: Close ghosts (dry run first!)
3. Verify: All ghosts closed

```bash
# 1. Identify
python scripts/cleanup_duplicate_positions.py --identify

# 2. Dry run
python scripts/cleanup_duplicate_positions.py --dry-run

# 3. Review output, then execute
python scripts/cleanup_duplicate_positions.py --execute

# 4. Verify
SELECT COUNT(*) FROM monitoring.positions WHERE status = 'active' AND id IN (581, 580, 590, 588, 587, 589);
# Should be 0
```

---

### Step 5: Monitoring (Day 4-5)

**Deploy**:
1. Fix 5.1: Add duplicate detection metric
2. Fix 5.2: Add periodic duplicate check

**Verify**:
```bash
# Check metrics
grep "duplicates_prevented" logs/production_bot_*.log

# Check periodic check
grep "DUPLICATE POSITIONS" logs/production_bot_*.log
```

---

## ‚úÖ SUCCESS CRITERIA

### Criteria #1: No New Duplicates

**Test**:
```sql
-- Run after 24 hours
SELECT
    COUNT(DISTINCT symbol) as unique_symbols,
    COUNT(*) as total_positions,
    COUNT(*) - COUNT(DISTINCT symbol) as duplicates
FROM monitoring.positions
WHERE status = 'active'
  AND created_at >= NOW() - INTERVAL '24 hours';
```

**Expected**: `duplicates = 0`

---

### Criteria #2: Existing Duplicates Cleaned

**Test**:
```sql
SELECT COUNT(*)
FROM monitoring.positions
WHERE id IN (581, 580, 590, 588, 587, 589)
  AND status = 'active';
```

**Expected**: `0` (all closed)

---

### Criteria #3: DB Constraint Active

**Test**:
```sql
-- Try to create duplicate (should fail)
INSERT INTO monitoring.positions (symbol, exchange, status, quantity, entry_price, side)
VALUES ('TESTUSDT', 'binance', 'active', 100, 1.0, 'long'),
       ('TESTUSDT', 'binance', 'active', 100, 1.0, 'long');
```

**Expected**: `ERROR: duplicate key value violates unique constraint`

---

### Criteria #4: Metrics Working

**Test**:
```python
# Check metrics endpoint or logs
position_manager.metrics['duplicates_prevented']
```

**Expected**: `> 0` if any duplicate attempts were made

---

## üìä RISK ASSESSMENT

| Fix | Risk Level | Impact if Failed | Mitigation |
|-----|------------|------------------|------------|
| Fix 1.1 | üü¢ LOW | Returns wrong ID | Test thoroughly |
| Fix 1.2 | üü° MEDIUM | Sync fails | Try-catch, fallback |
| Fix 2.1 | üü¢ LOW | Slower startup | Acceptable |
| Fix 2.2 | üü¢ LOW | Wrong symbols list | Log & verify |
| Fix 3.1 | üü° MEDIUM | Breaks inserts | Rollback plan ready |
| Fix 3.2 | üü¢ LOW | Exception unhandled | Try-catch |
| Fix 4.1-4.2 | üü° MEDIUM | Close wrong positions | Dry run first |
| Fix 5.1-5.2 | üü¢ LOW | Alerts spam | Tune thresholds |

---

## üéØ SUMMARY

### What We Know

1. ‚úÖ **Root causes identified**: 3 independent issues
2. ‚úÖ **–ú–∞—Å—à—Ç–∞–± quantified**: 6 –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö –ø–∞—Ä
3. ‚úÖ **Impact assessed**: Only DB pollution, no trading impact
4. ‚úÖ **Fixes designed**: 8 fixes across 5 phases
5. ‚úÖ **Plan ready**: Step-by-step implementation guide

### Critical Path

**Day 1**: Hotfix (Fixes 1.1, 1.2, 2.1, 2.2)
**Day 2**: DB Protection (Fixes 3.1, 3.2)
**Day 3**: Cleanup (Fixes 4.1, 4.2)
**Day 4**: Monitoring (Fixes 5.1, 5.2)
**Day 5**: Verification & Sign-off

### Next Actions

1. ‚è≠Ô∏è **Review this plan** with team
2. ‚è≠Ô∏è **Approve deployment** strategy
3. ‚è≠Ô∏è **Execute Step 1** (Testing)
4. ‚è≠Ô∏è **Deploy hotfix** (Step 2)
5. ‚è≠Ô∏è **Monitor & verify** (Steps 3-5)

---

**Plan Status**: ‚úÖ READY FOR REVIEW
**Estimated Time**: 5 days (with testing)
**Risk Level**: üü° MEDIUM (manageable with proper testing)
**Priority**: üî¥ HIGH (P1)

---

**Report Prepared By**: Claude Code Analysis System
**Date**: 2025-10-17
**Review Status**: ‚úÖ Ready for Engineering Review & Approval
**Next Step**: CODE REVIEW ‚Üí TESTING ‚Üí DEPLOYMENT
